<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Algorithmic Bias Explorer | Adam Langshaw</title>
  <link rel="icon" type="image/png" href="assets/favicon-ec29af6c-5854-4529-b155-a2d1de3ed322.png" />
  <link rel="stylesheet" href="style.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=DM+Serif+Display&family=Source+Sans+3:wght@400;600&display=swap"
    rel="stylesheet"
  />
  <style>
    :root {
      --bias-red: #8c1515;
      --bias-blue: #1d4ed8;
      --bias-gray: #6b7280;
      --bias-border: #e5e7eb;
      --bias-card: #f9fafb;
    }

    body {
      background: #ffffff;
    }

    .bias-header {
      background: linear-gradient(135deg, #111827 0%, #312e81 40%, #8c1515 100%);
      color: white;
      padding: 3rem 1.5rem;
      text-align: center;
      margin-bottom: 3rem;
    }

    .bias-header h1 {
      font-family: "DM Serif Display", serif;
      font-size: 2.5rem;
      margin: 0 0 0.5rem;
      font-weight: 400;
    }

    .bias-header p {
      max-width: 720px;
      margin: 0 auto;
      font-size: 1.05rem;
      opacity: 0.95;
    }

    .bias-container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 0 1.5rem 4rem;
    }

    .bias-nav {
      margin-bottom: 2rem;
      text-align: center;
    }

    .bias-nav a {
      color: var(--bias-red);
      font-weight: 600;
      text-decoration: none;
    }

    .bias-nav a:hover {
      text-decoration: underline;
    }

    .scenario-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(260px, 1fr));
      gap: 1.5rem;
      margin-bottom: 2.5rem;
    }

    .scenario-card {
      background: var(--bias-card);
      border-radius: 12px;
      border: 1px solid var(--bias-border);
      padding: 1.25rem 1.5rem;
      cursor: pointer;
      transition: box-shadow 0.2s, border-color 0.2s, transform 0.2s;
    }

    .scenario-card:hover {
      box-shadow: 0 4px 16px rgba(0, 0, 0, 0.08);
      border-color: var(--bias-blue);
      transform: translateY(-1px);
    }

    .scenario-card.selected {
      border-color: var(--bias-red);
      box-shadow: 0 4px 18px rgba(140, 21, 21, 0.15);
    }

    .scenario-card h3 {
      font-family: "DM Serif Display", serif;
      margin: 0 0 0.5rem;
      font-size: 1.25rem;
    }

    .scenario-card p {
      margin: 0;
      font-size: 0.95rem;
      color: var(--bias-gray);
    }

    .bias-layout {
      display: grid;
      grid-template-columns: minmax(0, 1.35fr) minmax(0, 1.1fr);
      gap: 2rem;
      align-items: stretch;
    }

    .controls-card,
    .metrics-card {
      background: #ffffff;
      border-radius: 12px;
      border: 1px solid var(--bias-border);
      padding: 1.75rem 1.5rem;
      box-shadow: 0 2px 10px rgba(0, 0, 0, 0.04);
      height: 100%;
      display: flex;
      flex-direction: column;
      justify-content: flex-start;
    }

    .controls-card h2,
    .metrics-card h2 {
      font-family: "DM Serif Display", serif;
      margin: 0 0 0.75rem;
      font-size: 1.5rem;
    }

    .controls-card p,
    .metrics-card p {
      margin: 0 0 0.75rem;
      font-size: 0.95rem;
      color: var(--bias-gray);
    }

    .group-controls {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(230px, 1fr));
      gap: 1.25rem;
      margin-top: 1rem;
    }

    .group-card {
      border-radius: 10px;
      border: 1px solid var(--bias-border);
      padding: 1rem 1.1rem;
      background: var(--bias-card);
    }

    .group-card h3 {
      margin: 0 0 0.35rem;
      font-size: 1rem;
      font-weight: 600;
    }

    .group-card small {
      display: block;
      margin-bottom: 0.5rem;
      font-size: 0.8rem;
      color: var(--bias-gray);
    }

    .slider-row {
      margin-bottom: 0.75rem;
    }

    .slider-row label {
      display: flex;
      justify-content: space-between;
      font-size: 0.85rem;
      margin-bottom: 0.25rem;
    }

    .slider-row input[type="range"] {
      width: 100%;
    }

    .metric-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(210px, 1fr));
      gap: 1rem;
      margin-top: 1rem;
    }

    .metric-box {
      border-radius: 10px;
      border: 1px solid var(--bias-border);
      padding: 0.9rem 1rem;
      background: var(--bias-card);
    }

    .metric-box h4 {
      margin: 0 0 0.35rem;
      font-size: 0.85rem;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      color: var(--bias-gray);
    }

    .metric-main {
      font-size: 1.3rem;
      font-weight: 600;
      margin: 0 0 0.15rem;
    }

    .metric-main.bad {
      color: var(--bias-red);
    }

    .metric-main.good {
      color: var(--bias-green, #15803d);
    }

    .metric-sub {
      margin: 0;
      font-size: 0.85rem;
      color: var(--bias-gray);
    }

    .legend {
      margin-top: 1.25rem;
      font-size: 0.82rem;
      color: var(--bias-gray);
    }

    .legend span {
      display: inline-block;
      margin-right: 0.75rem;
    }

    .dot-a,
    .dot-b {
      display: inline-block;
      width: 10px;
      height: 10px;
      border-radius: 999px;
      margin-right: 0.25rem;
    }

    .dot-a {
      background: var(--bias-blue);
    }

    .dot-b {
      background: var(--bias-red);
    }

    @media (max-width: 840px) {
      .bias-layout {
        grid-template-columns: minmax(0, 1fr);
      }
    }
  </style>
</head>
<body>
  <div class="bias-header">
    <h1>Algorithmic Bias Explorer</h1>
    <p>
      A toy model of how algorithms can treat groups differently even when the math “looks neutral.”
      Adjust thresholds and base rates for two groups and watch common fairness metrics shift.
    </p>
  </div>

  <div class="bias-container">
    <div class="bias-nav">
      <a href="index.html">← Back to Home</a>
    </div>

    <div class="scenario-grid" id="scenarioGrid">
      <div class="scenario-card selected" data-scenario="loans">
        <h3>Loan Approvals</h3>
        <p>Predict who will repay a loan. Explore how different approval thresholds change denial rates by group.</p>
      </div>
      <div class="scenario-card" data-scenario="hiring">
        <h3>Hiring Filter</h3>
        <p>Rank job applicants and decide who gets interviews. See how small shifts change who gets screened out.</p>
      </div>
      <div class="scenario-card" data-scenario="health">
        <h3>Health Risk Score</h3>
        <p>Flag “high-risk” patients for extra care. Examine who is over- or under-flagged across groups.</p>
      </div>
    </div>

    <div class="bias-layout">
      <section class="controls-card">
        <h2>Model &amp; Group Controls</h2>
        <p>
          Each group has a different <strong>base rate</strong> (how often the true outcome is present in that population)
          and a <strong>decision threshold</strong> (how strict the algorithm is about saying “yes”). In real systems,
          these differences can come from data quality, historical bias, or explicit policy choices.
        </p>

        <div class="group-controls">
          <div class="group-card">
            <h3><span class="dot-a"></span>Group A</h3>
            <small>
              Often the majority group in training data. Higher base rate = more people in this group truly have the
              positive outcome. Higher threshold = the model demands stronger evidence before saying “yes.”
            </small>
            <div class="slider-row">
              <label>
                Base Rate (true positives in population)
                <span id="baseRateALabel">40%</span>
              </label>
              <input
                type="range"
                id="baseRateA"
                min="5"
                max="80"
                value="40"
                oninput="updateBiasModel()"
              />
            </div>
            <div class="slider-row">
              <label>
                Decision Threshold
                <span id="thresholdALabel">0.50</span>
              </label>
              <input
                type="range"
                id="thresholdA"
                min="20"
                max="80"
                value="50"
                oninput="updateBiasModel()"
              />
            </div>
          </div>

          <div class="group-card">
            <h3><span class="dot-b"></span>Group B</h3>
            <small>
              Often underrepresented or historically marginalized. Different base rates and thresholds here simulate
              under‑measurement, noisier data, or stricter rules for this group.
            </small>
            <div class="slider-row">
              <label>
                Base Rate
                <span id="baseRateBLabel">40%</span>
              </label>
              <input
                type="range"
                id="baseRateB"
                min="5"
                max="80"
                value="40"
                oninput="updateBiasModel()"
              />
            </div>
            <div class="slider-row">
              <label>
                Decision Threshold
                <span id="thresholdBLabel">0.60</span>
              </label>
              <input
                type="range"
                id="thresholdB"
                min="20"
                max="80"
                value="60"
                oninput="updateBiasModel()"
              />
            </div>
          </div>
        </div>

        <p class="legend">
          <span><span class="dot-a"></span>Group A</span>
          <span><span class="dot-b"></span>Group B</span>
          <span>We simulate 10,000 people per group and approximate fairness metrics from that toy data.</span>
        </p>
      </section>

      <section class="metrics-card">
        <h2>Fairness Metrics</h2>
        <p id="scenarioDescription">
          In this scenario, a positive prediction means “approved for a loan.” Ideally, we want equal true positive rates
          and false positive rates across groups, but that rarely happens automatically.
        </p>

        <div class="metric-grid">
          <div class="metric-box">
            <h4>Selection Rate</h4>
            <p class="metric-main" id="selectionRateMain">—</p>
            <p class="metric-sub" id="selectionRateSub">Share of each group getting a positive prediction.</p>
          </div>
          <div class="metric-box">
            <h4>Equal Opportunity Gap</h4>
            <p class="metric-main bad" id="eoGapMain">—</p>
            <p class="metric-sub" id="eoGapSub">Difference in true positive rates (TPR) between groups.</p>
          </div>
          <div class="metric-box">
            <h4>False Positive Gap</h4>
            <p class="metric-main bad" id="fpGapMain">—</p>
            <p class="metric-sub" id="fpGapSub">Difference in false positive rates (FPR) between groups.</p>
          </div>
          <div class="metric-box">
            <h4>Who Pays the Error Cost?</h4>
            <p class="metric-main" id="errorBurdenMain">—</p>
            <p class="metric-sub" id="errorBurdenSub">
              Which group shoulders more “unfair” errors (false negatives or false positives), given this scenario.
            </p>
          </div>
        </div>
      </section>
    </div>
  </div>

  <script>
    const scenarios = {
      loans: {
        id: "loans",
        description:
          "Positive = “approved for a loan.” False negatives mean qualified applicants are denied; false positives mean risky loans are approved.",
        errorPreference:
          "Here we usually worry more about false negatives for marginalized groups (qualified but denied).",
      },
      hiring: {
        id: "hiring",
        description:
          "Positive = “invited to interview.” False negatives mean strong candidates are filtered out; false positives mean weaker candidates get through.",
        errorPreference:
          "False negatives weigh heavily here: a group with more false negatives may be shut out of opportunities.",
      },
      health: {
        id: "health",
        description:
          "Positive = “flagged as high-risk and given extra care.” False negatives mean high-risk patients are missed; false positives mean low-risk patients are overtreated.",
        errorPreference:
          "In health risk, missing high-risk patients (false negatives) can be far more harmful than over-flagging.",
      },
    };

    let currentScenario = "loans";

    document.querySelectorAll(".scenario-card").forEach((card) => {
      card.addEventListener("click", () => {
        document.querySelectorAll(".scenario-card").forEach((c) => c.classList.remove("selected"));
        card.classList.add("selected");
        currentScenario = card.dataset.scenario;
        document.getElementById("scenarioDescription").textContent =
          scenarios[currentScenario].description + " " + scenarios[currentScenario].errorPreference;
        updateBiasModel();
      });
    });

    function simulateGroup(baseRatePct, thresholdPct) {
      const baseRate = baseRatePct / 100;
      const threshold = thresholdPct / 100;
      const n = 10000;

      let tp = 0;
      let fp = 0;
      let tn = 0;
      let fn = 0;

      for (let i = 0; i < n; i++) {
        const isPositive = Math.random() < baseRate;
        // Simulate a score: if truly positive, scores are higher on average
        const mean = isPositive ? 0.7 : 0.3;
        const score = clamp01(mean + (Math.random() - 0.5) * 0.6);
        const predictedPositive = score >= threshold;

        if (isPositive && predictedPositive) tp++;
        else if (isPositive && !predictedPositive) fn++;
        else if (!isPositive && predictedPositive) fp++;
        else tn++;
      }

      const selectionRate = (tp + fp) / n;
      const tpr = tp / (tp + fn || 1);
      const fpr = fp / (fp + tn || 1);

      return { selectionRate, tpr, fpr, tp, fp, tn, fn };
    }

    function clamp01(x) {
      return Math.min(1, Math.max(0, x));
    }

    function pct(x) {
      return (x * 100).toFixed(1) + "%";
    }

    function updateBiasModel() {
      const baseRateA = parseInt(document.getElementById("baseRateA").value, 10);
      const baseRateB = parseInt(document.getElementById("baseRateB").value, 10);
      const thresholdA = parseInt(document.getElementById("thresholdA").value, 10);
      const thresholdB = parseInt(document.getElementById("thresholdB").value, 10);

      document.getElementById("baseRateALabel").textContent = baseRateA + "%";
      document.getElementById("baseRateBLabel").textContent = baseRateB + "%";
      document.getElementById("thresholdALabel").textContent = (thresholdA / 100).toFixed(2);
      document.getElementById("thresholdBLabel").textContent = (thresholdB / 100).toFixed(2);

      const statsA = simulateGroup(baseRateA, thresholdA);
      const statsB = simulateGroup(baseRateB, thresholdB);

      // Selection rate display (both groups)
      const selA = pct(statsA.selectionRate);
      const selB = pct(statsB.selectionRate);
      document.getElementById("selectionRateMain").textContent = `${selA} vs ${selB}`;
      document.getElementById(
        "selectionRateSub"
      ).textContent = `Group A vs Group B share receiving a positive decision.`;

      // Equal opportunity (TPR) gap
      const eoGap = (statsA.tpr - statsB.tpr) * 100;
      const eoAbs = Math.abs(eoGap).toFixed(1);
      document.getElementById("eoGapMain").textContent = eoAbs + " pts";
      document.getElementById(
        "eoGapSub"
      ).textContent = `Difference in true positive rates — ${pct(statsA.tpr)} vs ${pct(statsB.tpr)}.`;

      // False positive gap
      const fpGap = (statsA.fpr - statsB.fpr) * 100;
      const fpAbs = Math.abs(fpGap).toFixed(1);
      document.getElementById("fpGapMain").textContent = fpAbs + " pts";
      document.getElementById(
        "fpGapSub"
      ).textContent = `Difference in false positive rates — ${pct(statsA.fpr)} vs ${pct(statsB.fpr)}.`;

      // Error burden: which group shoulders more of the "unfair" errors?
      const weightFalseNegatives = currentScenario === "health" ? 3 : 2;
      const weightFalsePositives = 1;

      const burdenA =
        statsA.fn * weightFalseNegatives + statsA.fp * weightFalsePositives;
      const burdenB =
        statsB.fn * weightFalseNegatives + statsB.fp * weightFalsePositives;

      let burdenText = "Roughly balanced";
      if (burdenA > burdenB * 1.1) {
        burdenText = "Group A bears more of the harmful errors.";
      } else if (burdenB > burdenA * 1.1) {
        burdenText = "Group B bears more of the harmful errors.";
      }

      const totalErrors = burdenA + burdenB || 1;
      const shareA = ((burdenA / totalErrors) * 100).toFixed(1);
      const shareB = ((burdenB / totalErrors) * 100).toFixed(1);

      document.getElementById("errorBurdenMain").textContent = `${shareA}% vs ${shareB}%`;
      document.getElementById("errorBurdenSub").textContent = burdenText;
    }

    // Initialize on load
    updateBiasModel();
  </script>
</body>
</html>

